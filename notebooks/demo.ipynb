{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e010e381f4d1bbe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 🤺 Fence demo\n",
    "\n",
    "This notebook demonstrates the use of various classes in this package. The core intent of this package is to provide a framework for interacting with language models in a way that is modular, extensible, and easy to use. The core classes are:\n",
    "- `PromptTemplate`: A template for a prompt that can be rendered with a dictionary of variables (or using keywords). It wraps around jinja2.Template, but adds some additional functionality.\n",
    "- `LLM`: A wrapper around a language model. \n",
    "- `Link`: Atomic LLM interaction wrapper. Takes in a PromptTemplate and a language model. It can be invoked with a dictionary of variables, and will return a dictionary of variables.\n",
    "- `TransformationLink`: A wrapper around a `function` that takes a dictionary of variables and returns a dictionary of variables. It can be invoked with a dictionary of variables, and will return a dictionary of variables.\n",
    "- `Chain`: A collection of links that are invoked in the right order based on the input and output keys for each Link.\n",
    "- `LinearChain`: A sequence of links that are invoked in the order they are passed in.\n",
    "\n",
    "Through these classes, we can interact with LLMs in varying degrees of complexity:\n",
    "1. Just call the model directly\n",
    "2. Use a `PromptTemplate` for reusability and abstraction\n",
    "3. Use a `Link` for atomic LLM interaction\n",
    "4. Use a `Chain` to execute a collection of `Links`\n",
    "\n",
    "Below, we'll go through each of these levels of complexity and provide examples.\n",
    "\n",
    "**Note** This notebook assumes you have access to AWS Bedrock, as we use Bedrock's Claude-instant model to fuel our LLM interactions."
   ]
  },
  {
   "cell_type": "code",
   "id": "a937d38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T11:33:38.502792Z",
     "start_time": "2024-07-11T11:33:38.496493Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "current_dir = Path('.').resolve().parents[0]\n",
    "import sys\n",
    "sys.path.append(str(current_dir))"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T11:33:38.538619Z",
     "start_time": "2024-07-11T11:33:38.534456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "from fence.models.claude import ClaudeInstant\n",
    "from fence.models.claude3 import Claude35Sonnet\n",
    "from fence.models.gpt import GPT4o\n",
    "from fence.templates.string import StringTemplate\n",
    "from fence.templates.messages import MessagesTemplate, Messages, Message\n",
    "from fence.links import Link, TransformationLink\n",
    "from fence.chains import Chain, LinearChain"
   ],
   "id": "d5a08a1007e6fa35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ⚙️ Setting up",
   "id": "1aa24e1159152eb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T11:33:38.590793Z",
     "start_time": "2024-07-11T11:33:38.539425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get our model\n",
    "model = Claude35Sonnet(source='demo_notebook', region='us-east-1')\n",
    "model = GPT4o(source='demo_notebook')"
   ],
   "id": "a61082844097188f",
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "efabe783b73b8ec7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 🪨 Level 1 - Just call the damn thing"
   ]
  },
  {
   "cell_type": "code",
   "id": "950662b95a79575c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-07-11T11:33:46.544111Z",
     "start_time": "2024-07-11T11:33:38.591446Z"
    }
   },
   "source": [
    "# Use the invoke method to call the model\n",
    "model.invoke('Why is the sky blue?')"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by bedrock service: 'Claude35Sonnet' object has no attribute 'invocation_logging'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/Repositories/showpad_personal/fence/fence/models/claude3.py:201\u001B[0m, in \u001B[0;36mClaude3Base._invoke\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;66;03m# Log invocation\u001B[39;00m\n\u001B[0;32m--> 201\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvocation_logging\u001B[49m()\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Claude35Sonnet' object has no attribute 'invocation_logging'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Use the invoke method to call the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mWhy is the sky blue?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Repositories/showpad_personal/fence/fence/models/claude3.py:117\u001B[0m, in \u001B[0;36mClaude3Base.invoke\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03mCall the model with the given prompt\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m:param prompt: text to feed the model\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;124;03m:return: response\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# Call the model\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# Get response completion\u001B[39;00m\n\u001B[1;32m    120\u001B[0m response_body \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mdecode())\n",
      "File \u001B[0;32m~/Documents/Repositories/showpad_personal/fence/fence/models/claude3.py:206\u001B[0m, in \u001B[0;36mClaude3Base._invoke\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError raised by bedrock service: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Error raised by bedrock service: 'Claude35Sonnet' object has no attribute 'invocation_logging'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "22c4da28c3c3d3b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Just call the damn thing\n",
    "model('Why is the sky blue?')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "632046188100875d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 🔨 Level 2 - Use a PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "id": "afe2b6d1d4078270",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Initialize a prompt template\n",
    "prompt_template = StringTemplate('Why is the sky {color}?')\n",
    "print(prompt_template)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9901374952217046",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Render it with a dictionary\n",
    "print(prompt_template.render({'color': 'blue'}))\n",
    "\n",
    "# Render it with keyword arguments\n",
    "print(prompt_template.render(color='red'))\n",
    "\n",
    "# Input dict takes precedence over keyword arguments\n",
    "print(prompt_template.render(input_dict={'color': 'blue'}, color='red'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5dca91633a69af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# You can concatenate prompt templates, input variables are merged\n",
    "prompt_template_sky = StringTemplate('Why is the sky {color}?')\n",
    "prompt_template_grass = StringTemplate('Why is the grass {color}?')\n",
    "prompt_template_dress = StringTemplate('I like a dress with {pattern}.')\n",
    "combined_prompt_template = prompt_template_sky + prompt_template_grass + prompt_template_dress\n",
    "print(combined_prompt_template)\n",
    "print(combined_prompt_template.render({'color': 'blue', 'pattern': 'polka dots'}))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d7766d8c7a53b12",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# You can customize the separator\n",
    "base_template = StringTemplate('Why is the sky {color}?', separator=' FUNKY TOWN ')\n",
    "additional_template = StringTemplate('Why is the grass {color}?')\n",
    "print((base_template + additional_template).render({'color': 'blue'}))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You can also use the MessagesTemplates for more complex prompts\n",
    "messages = Messages(\n",
    "    system='Respond in a {tone} tone',\n",
    "    messages= [\n",
    "        Message(role=\"user\", content=\"Why is the sky {color}?\"),\n",
    "        # Equivalent to Message(role='user', content=Content(type='text', text='Why is the sky blue?'))\n",
    "        # But Content can also be an image, etc.\n",
    "    ]\n",
    ")\n",
    "messages_template = MessagesTemplate(\n",
    "    source=messages\n",
    ")\n",
    "print(messages_template)\n",
    "print(f\"\\nRendered: {messages_template.render(tone='sarcastic', color='blue')}\")"
   ],
   "id": "c6c3140b13ae4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e5617e437fb30fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 🧠 Level 3 - Use Links (lol)\n",
    "\n",
    "What are Links? In this context, they represent atomic components of LLM interaction. That means they should be able to be strung together to form a Chain, although they can be used independently as well."
   ]
  },
  {
   "cell_type": "code",
   "id": "219ef991de4352a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# The simplest link is the Link class, which just takes a prompt template and a model\n",
    "link = Link(template=prompt_template, llm=model)\n",
    "print(link)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a1279bdd677c524",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Invoke it\n",
    "link(color='blue') # Or, equivalently, link.run(input_dict={'color': 'blue'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "884bda0b4dca2512",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# By default, output is stored under 'state'. You can get a copy (e.g., for inspection of intermediate results) by passing a different output key\n",
    "link = Link(template=prompt_template, llm=model, output_key='intermediate')\n",
    "link.run(input_dict={'color': 'blue'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52913f8c6dece54e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# You can name your links for easier debugging in logs\n",
    "link = Link(template=prompt_template, llm=model, name='sky')\n",
    "link.run(input_dict={'color': 'blue'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "460c06d5b33c6e87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# You can also build TransformationLinks, which take a function that transforms any input_dict into a specific output\n",
    "def concatenate(input: dict):\n",
    "    return f\"{input['X']} and {input['Y']}\"\n",
    "\n",
    "concat_link = TransformationLink(\n",
    "    input_keys=[\"X\", \"Y\"], function=concatenate, output_key=\"C\"\n",
    ")\n",
    "\n",
    "concat_link.run(input_dict={\"X\": \"Hello\", \"Y\": \"World\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c83119613a5ecf54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 🚀 Level 4 - Use Chains (lol again)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a86436d806745ef0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# You can also build Chains, which are just a sequence of links. There are two types of chains: LinearChain and Chain. \n",
    "# LinearChain is a sequence of links, while Chain is a collection of links that are invoked in the right order based on the input and output keys for each Link.\n",
    "\n",
    "# This is a LinearChain #\n",
    "#########################\n",
    "\n",
    "# Build some links\n",
    "link_opposite = Link(\n",
    "    template=StringTemplate(\n",
    "        \"What's the opposite of {A}? Reply with a few words max.\"\n",
    "    ),\n",
    "    name = 'opposite',\n",
    "    output_key=\"X\",\n",
    ")\n",
    "link_synonym = Link(\n",
    "    template=StringTemplate(\n",
    "        \"What's a synonym for {B}. Reply with one word.\"\n",
    "    ),\n",
    "    name='synonym',\n",
    "    output_key=\"Y\",\n",
    ")\n",
    "link_poem = Link(\n",
    "    template=StringTemplate(\n",
    "        \"Write a poem about {state}. Return only the poem, beginning with the title.\"\n",
    "    ),\n",
    "    name='poem',\n",
    "    output_key=\"Z\",\n",
    ")\n",
    "\n",
    "# Now build a LinearChain\n",
    "linear_chain = LinearChain(llm=model, links=[link_opposite, link_synonym, concat_link, link_poem])\n",
    "\n",
    "# Run it\n",
    "result = linear_chain.run(input_dict={\"A\": \"A police officer\", \"B\": \"Hopeful\"})\n",
    "\n",
    "# Get the output\n",
    "print(result['state'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d1a3dc176bd9580",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# A LinearChain will take the presence of the 'state' key into account when invoking the next link.\n",
    "# A Chain will not. However, it has an extra 'feature' in the form of topological sorting. As long as a graph of links can be\n",
    "# extracted from the chain, and the input keys (that are not generated in the chain) are given, the chain will invoke the links in the right order.\n",
    "\n",
    "# This is a Chain #\n",
    "###################\n",
    "link_a = Link(\n",
    "    template=StringTemplate(\n",
    "        \"Capitalize this word: {A}. Only respond with the capitalized version\",\n",
    "    ),\n",
    "    name = 'opposite',\n",
    "    output_key=\"X\",\n",
    ")\n",
    "link_b = Link(\n",
    "    template=StringTemplate(\n",
    "        \"What's a synonym for {B}. Reply with one word.\", \n",
    "    ),\n",
    "    name='superlative',\n",
    "    output_key=\"Y\",\n",
    ")\n",
    "link_c = Link(\n",
    "    template=StringTemplate(\n",
    "        \"Combine {X} and {Y} and {C} in a meaningful sentence.\", \n",
    "    ),\n",
    "    name='sentence',\n",
    "    output_key=\"Z\",\n",
    ")\n",
    "chain = Chain(llm=model, links=[link_c, link_a, link_b]) # Note that we can pass the links in any order\n",
    "\n",
    "# This is the sorted graph of links\n",
    "chain._topological_sort()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5fca5fd6f655679",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Now we can run it\n",
    "try:\n",
    "    result = chain.run(input_dict={\"A\": \"A police officer\", \"B\": \"Hopeful\"})\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4dabac4f53177f1e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Woops, forgot something! There's no link that generates the 'C' key. We can pass it in though.\n",
    "result = chain.run(input_dict={\"A\": \"A police officer\", \"B\": \"Hopeful\", \"C\": \"a dog\"})\n",
    "print(result['state'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd6aa1cb2325cd41",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Cycles are not allowed\n",
    "link_up = Link(\n",
    "    template=StringTemplate(\n",
    "        \"Capitalize this word: {up}. Only respond with the capitalized version\",\n",
    "    ),\n",
    "    name = 'up',\n",
    "    output_key=\"down\",\n",
    ")\n",
    "link_b = Link(\n",
    "    template=StringTemplate(\n",
    "        \"What's a synonym for {down}. Reply with one word.\", \n",
    "    ),\n",
    "    name='down',\n",
    "    output_key=\"up\",\n",
    ")\n",
    "chain = Chain(llm=model, links=[link_up, link_b])\n",
    "try:\n",
    "    chain._topological_sort()\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3a8561144f1584c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "858e4883-dd26-4d88-a009-d0145fafbfa6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
